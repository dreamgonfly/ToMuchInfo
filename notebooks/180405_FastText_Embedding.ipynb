{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 180405_FastText_Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/movie.txt\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [str(t.strip()) for t in data]\n",
    "    data = pd.DataFrame(data, columns=['comment'])\n",
    "    \n",
    "with open(\"../data/movie_label.txt\") as f:\n",
    "    score = f.readlines()\n",
    "    data['score'] = pd.DataFrame(score)\n",
    "    data['score'] = data['score'].map(lambda x: int(x.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 3.77 s, total: 1min 4s\n",
      "Wall time: 29.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import fasttext\n",
    "model = fasttext.skipgram(\"../data/movie_tokenized_twitter.txt\", 'model')\n",
    "len(model.words) # 전체 개별 토큰들(3-6 characters)\n",
    "len(model['비웃']) # token넣으면 vector 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.skipgram(\"../data/movie_tokenized_twitter.txt\", 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_extractors import LengthFeatureExtractor, ImportantWordFeaturesExtractor, MovieActorFeaturesExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, validate_set = train_test_split(data, test_size=0.1, stratify=data['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/movie_tokenized_twitter.txt\", ) as f:\n",
    "    tokenized_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenized_data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Strange :  100\n",
      "6000\n",
      "7000\n",
      "Strange :  200\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "Strange :  300\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "Strange :  400\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "Strange :  500\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "Strange :  600\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "Strange :  700\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "Strange :  800\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "Strange :  900\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "Strange :  1000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "Strange :  1100\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "Strange :  1200\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "Strange :  1300\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "Strange :  1400\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "Strange :  1500\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "Strange :  1600\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "Strange :  1700\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "Strange :  1800\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "Strange :  1900\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "Strange :  2000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "Strange :  2100\n",
      "98000\n",
      "99000\n"
     ]
    }
   ],
   "source": [
    "for i,line in enumerate(tokenized_data):\n",
    "    sentence_vectors = []\n",
    "    string = line.strip().replace('\\n','')\n",
    "    for token in string.split():\n",
    "        sentence_vectors.append(model[token])\n",
    "\n",
    "    if len(sentence_vectors):\n",
    "        sentence_vectors = np.mean(np.array(sentence_vectors).T, axis=1)\n",
    "    else:\n",
    "        sentence_vectors = np.zeros((1,100))\n",
    "        \n",
    "    if i==0:\n",
    "        X = sentence_vectors.reshape(1,-1)\n",
    "    else : \n",
    "        X = np.concatenate((X,sentence_vectors.reshape(1,-1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
