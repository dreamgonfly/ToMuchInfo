{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 180408_FastTextVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì§€ê¸ˆ FastTextVectorizer + WordCNNì˜ ë¬¸ì œëŠ” WordCNNì— ë§ê²Œ ì ì ˆí•œ tokenizingì´ ë˜ì§€ ì•Šì•˜ë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë”°ë¼ì„œ TwitterTokenizerì˜ í† í°ë³„ë¡œ embeddingì„ í•˜ëŠ” vectorizerì™€ ê°ê°ì˜ ìëª¨ë³„ë¡œ embeddingì„ í•˜ì—¬ ê·¸ í† í°ì— ë¶™ì´ëŠ” í˜•ì‹ìœ¼ë¡œ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- TwitterTokenizer : ì–˜ëŠ” ì¼ë‹¨ ì˜í™”ì™€ ë°°ìš°ë¥¼ maskingí•˜ê³  ë‚˜ë¨¸ì§€ëŠ” Pos tagging í•˜ê² ìŠµë‹ˆë‹¤. returnê°’ì´ ë‹¤ë¥¸ ê²ƒë“¤ê³¼ ë‹¤ë¦…ë‹ˆë‹¤. \n",
    "- FastTextTokenVectorizer : FastTextê¸°ë°˜ìœ¼ë¡œ tokenì„ í•™ìŠµí•˜ì—¬ vectorë¡œ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\uf431', 'Foreign'),\n",
       " ('ê½ƒ', 'Noun'),\n",
       " ('ì´', 'Josa'),\n",
       " ('í”¼ë‹¤', 'Verb'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Twitter().pos(\"ğŸ±ê½ƒì´ í”¼ì—ˆìŠµë‹ˆë‹¤.\", stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterTokenizer:\n",
    "    \"\"\"Split text to twitter based tokens\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.twitter = Twitter()\n",
    "        self.mv = re.compile(r'mv[0-9]{2,10}')\n",
    "        self.ac = re.compile(r'ac[0-9]{2,10}') \n",
    "    \n",
    "    def tokenize(self, raw_text, stem=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_text: \"ë¬´ê¶í™” ê½ƒì´ í”¼ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "        Returns:\n",
    "            ë¨¼ì € ì˜í™”idì™€ ë°°ìš°idë¥¼ masking\n",
    "            A list of (token, pos) : [(\"ë¬´ê¶í™”\",\"Noun\"), (\"ê½ƒ\",\"Noun\")...] \n",
    "        \"\"\"\n",
    "        mv_replaced = self.mv.sub('ğŸ¶', raw_text)\n",
    "        ac_replaced = self.ac.sub('ğŸ±', mv_replaced)\n",
    "        tokenized_text = self.twitter.pos(ac_replaced, stem=stem)\n",
    "        idx_mv = []\n",
    "        idx_ac = []\n",
    "        for i, (token, pos) in enumerate(tokenized_text):\n",
    "            if token=='\\uf436':\n",
    "                idx_mv.append(i)\n",
    "            elif token=='\\uf431':\n",
    "                idx_ac.append(i)\n",
    "                \n",
    "        for i in idx_mv:\n",
    "            tokenized_text[i] = ('ğŸ¶', 'Movie')\n",
    "        for i in idx_ac:\n",
    "            tokenized_text[i] = ('ğŸ±', 'Actor')\n",
    "            \n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TwitterTokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0d26b7c90a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwitterTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFastTextVectorizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\"A dictionary that maps a word to FastText embedding.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TwitterTokenizer'"
     ]
    }
   ],
   "source": [
    "from tokenizers import TwitterTokenizer\n",
    "\n",
    "class FastTextVectorizer:\n",
    "    \"\"\"A dictionary that maps a word to FastText embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, config):\n",
    "        self.tokenizer = TwitterTokenizer(config)\n",
    "        self.vocabulary_size = config.vocabulary_size\n",
    "        self.embedding_size = config.embedding_size\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.fasttext = None\n",
    "\n",
    "    def build_dictionary(self, data):\n",
    "\n",
    "        self.vocab_words, self.word2idx, self.idx2word = self._build_vocabulary(data)\n",
    "        self.embedding = self.load_vectors()\n",
    "        print(self.embedding.shape)\n",
    "\n",
    "    def indexer(self, word):\n",
    "        try:\n",
    "            return self.word2idx[word]\n",
    "        except:\n",
    "            return self.word2idx['<UNK>']\n",
    "\n",
    "    def _build_vocabulary(self, data):\n",
    "        reviews = [review for review, label in data]\n",
    "        tokenized_reviews = [self.tokenizer.tokenize(review, stem=False) for review in reviews]\n",
    "\n",
    "        tokens = [[token for token, pos in tokenized_list] for tokenized_list in tokenized_reviews]\n",
    "        tags = [[pos for token, pos in tokenized_list] for tokenized_list in tokenized_reviews]\n",
    "\n",
    "        self.fasttext = FastText(sentences=tokens,\n",
    "                                 size=self.embedding_size,\n",
    "                                 max_vocab_size=self.vocabulary_size - 2)\n",
    "\n",
    "        vocab_words = self.fasttext.wv.vocab\n",
    "        word2idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "        word2idx['<UNK>'] = len(vocab_words)\n",
    "        word2idx['<PAD>'] = len(vocab_words) + 1\n",
    "\n",
    "        idx2word = {idx: word for idx, word in enumerate(vocab_words)}\n",
    "        idx2word[len(vocab_words)] = '<UNK>'\n",
    "        idx2word[len(vocab_words) + 1] = '<PAD>'\n",
    "\n",
    "        return vocab_words, word2idx, idx2word\n",
    "\n",
    "    def load_vectors(self):\n",
    "        word_vectors = []\n",
    "        vocab_num = len(self.vocab_words)\n",
    "        for i in range(self.vocabulary_size):\n",
    "            if i < vocab_num:\n",
    "                word = self.idx2word[i]\n",
    "                if word in ['<UNK>', '<PAD>']:\n",
    "                    vector = np.zeros(self.embedding_size)\n",
    "                else:\n",
    "                    vector = self.fasttext.wv[word]\n",
    "                word_vectors.append(vector)\n",
    "            else:\n",
    "                word_vectors.append(np.zeros(self.embedding_size))\n",
    "        embedding = np.stack(word_vectors)\n",
    "        return embedding\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = {'idx2word': self.idx2word,\n",
    "                 'word2idx': self.word2idx,\n",
    "                 'vocab_words': self.vocab_words,\n",
    "                 'embedding': self.embedding.tolist()}\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.idx2word = state_dict['idx2word']\n",
    "        self.word2idx = state_dict['word2idx']\n",
    "        self.vocab_words = state_dict['vocab_words']\n",
    "        self.embedding = np.array(state_dict['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
