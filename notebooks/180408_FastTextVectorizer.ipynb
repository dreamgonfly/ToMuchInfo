{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 180408_FastTextVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금 FastTextVectorizer + WordCNN의 문제는 WordCNN에 맞게 적절한 tokenizing이 되지 않았다는 점입니다. 따라서 TwitterTokenizer의 토큰별로 embedding을 하는 vectorizer와 각각의 자모별로 embedding을 하여 그 토큰에 붙이는 형식으로 하겠습니다.\n",
    "\n",
    "- TwitterTokenizer : 얘는 일단 영화와 배우를 masking하고 나머지는 Pos tagging 하겠습니다. return값이 다른 것들과 다릅니다. \n",
    "- FastTextTokenVectorizer : FastText기반으로 token을 학습하여 vector로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\uf431', 'Foreign'),\n",
       " ('꽃', 'Noun'),\n",
       " ('이', 'Josa'),\n",
       " ('피다', 'Verb'),\n",
       " ('.', 'Punctuation')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Twitter().pos(\"🐱꽃이 피었습니다.\", stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterTokenizer:\n",
    "    \"\"\"Split text to twitter based tokens\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.twitter = Twitter()\n",
    "        self.mv = re.compile(r'mv[0-9]{2,10}')\n",
    "        self.ac = re.compile(r'ac[0-9]{2,10}') \n",
    "    \n",
    "    def tokenize(self, raw_text, stem=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_text: \"무궁화 꽃이 피었습니다.\"\n",
    "        Returns:\n",
    "            먼저 영화id와 배우id를 masking\n",
    "            A list of (token, pos) : [(\"무궁화\",\"Noun\"), (\"꽃\",\"Noun\")...] \n",
    "        \"\"\"\n",
    "        mv_replaced = self.mv.sub('🐶', raw_text)\n",
    "        ac_replaced = self.ac.sub('🐱', mv_replaced)\n",
    "        tokenized_text = self.twitter.pos(ac_replaced, stem=stem)\n",
    "        idx_mv = []\n",
    "        idx_ac = []\n",
    "        for i, (token, pos) in enumerate(tokenized_text):\n",
    "            if token=='\\uf436':\n",
    "                idx_mv.append(i)\n",
    "            elif token=='\\uf431':\n",
    "                idx_ac.append(i)\n",
    "                \n",
    "        for i in idx_mv:\n",
    "            tokenized_text[i] = ('🐶', 'Movie')\n",
    "        for i in idx_ac:\n",
    "            tokenized_text[i] = ('🐱', 'Actor')\n",
    "            \n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TwitterTokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-0d26b7c90a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTwitterTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFastTextVectorizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"\"\"A dictionary that maps a word to FastText embedding.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TwitterTokenizer'"
     ]
    }
   ],
   "source": [
    "from tokenizers import TwitterTokenizer\n",
    "\n",
    "class FastTextVectorizer:\n",
    "    \"\"\"A dictionary that maps a word to FastText embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, config):\n",
    "        self.tokenizer = TwitterTokenizer(config)\n",
    "        self.vocabulary_size = config.vocabulary_size\n",
    "        self.embedding_size = config.embedding_size\n",
    "        self.PAD_TOKEN = '<PAD>'\n",
    "        self.UNK_TOKEN = '<UNK>'\n",
    "        self.fasttext = None\n",
    "\n",
    "    def build_dictionary(self, data):\n",
    "\n",
    "        self.vocab_words, self.word2idx, self.idx2word = self._build_vocabulary(data)\n",
    "        self.embedding = self.load_vectors()\n",
    "        print(self.embedding.shape)\n",
    "\n",
    "    def indexer(self, word):\n",
    "        try:\n",
    "            return self.word2idx[word]\n",
    "        except:\n",
    "            return self.word2idx['<UNK>']\n",
    "\n",
    "    def _build_vocabulary(self, data):\n",
    "        reviews = [review for review, label in data]\n",
    "        tokenized_reviews = [self.tokenizer.tokenize(review, stem=False) for review in reviews]\n",
    "\n",
    "        tokens = [[token for token, pos in tokenized_list] for tokenized_list in tokenized_reviews]\n",
    "        tags = [[pos for token, pos in tokenized_list] for tokenized_list in tokenized_reviews]\n",
    "\n",
    "        self.fasttext = FastText(sentences=tokens,\n",
    "                                 size=self.embedding_size,\n",
    "                                 max_vocab_size=self.vocabulary_size - 2)\n",
    "\n",
    "        vocab_words = self.fasttext.wv.vocab\n",
    "        word2idx = {word: idx for idx, word in enumerate(vocab_words)}\n",
    "        word2idx['<UNK>'] = len(vocab_words)\n",
    "        word2idx['<PAD>'] = len(vocab_words) + 1\n",
    "\n",
    "        idx2word = {idx: word for idx, word in enumerate(vocab_words)}\n",
    "        idx2word[len(vocab_words)] = '<UNK>'\n",
    "        idx2word[len(vocab_words) + 1] = '<PAD>'\n",
    "\n",
    "        return vocab_words, word2idx, idx2word\n",
    "\n",
    "    def load_vectors(self):\n",
    "        word_vectors = []\n",
    "        vocab_num = len(self.vocab_words)\n",
    "        for i in range(self.vocabulary_size):\n",
    "            if i < vocab_num:\n",
    "                word = self.idx2word[i]\n",
    "                if word in ['<UNK>', '<PAD>']:\n",
    "                    vector = np.zeros(self.embedding_size)\n",
    "                else:\n",
    "                    vector = self.fasttext.wv[word]\n",
    "                word_vectors.append(vector)\n",
    "            else:\n",
    "                word_vectors.append(np.zeros(self.embedding_size))\n",
    "        embedding = np.stack(word_vectors)\n",
    "        return embedding\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = {'idx2word': self.idx2word,\n",
    "                 'word2idx': self.word2idx,\n",
    "                 'vocab_words': self.vocab_words,\n",
    "                 'embedding': self.embedding.tolist()}\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.idx2word = state_dict['idx2word']\n",
    "        self.word2idx = state_dict['word2idx']\n",
    "        self.vocab_words = state_dict['vocab_words']\n",
    "        self.embedding = np.array(state_dict['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
